{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insumeta/2024_AI/blob/main/20240309_mountain%20car/blackjack/20240309%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5_%EC%82%AC%EC%82%AC%EB%B0%98_%ED%95%99%EC%8A%B5%EC%9E%90%EB%A3%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2BsaXtWqpAp"
      },
      "source": [
        "# 강화 학습 맛보기\n",
        "\n",
        "강화 학습 수업에 오신 여러분을 진심으로 환영합니다!\n",
        "\n",
        "여러분이 앞으로 1년 동안 배우게 될 강화 학습은 인공지능 분야 중에서도 가장 독특하고 유용한 기술입니다.\n",
        "아마 다른 어떤 수업에서도 경험하지 못할 재미있고 유익한 경험이 될 거에요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPaT76WHqrUV"
      },
      "source": [
        "## 강화 학습 내용 훑어보기\n",
        "\n",
        "강화 학습은 시행착오를 거치며 경험을 통해 적절한 행동 방식을 배워나가는 과정입니다. \"당근과 채찍\" 전략을 생각하면 쉽습니다. 여러분이 좋은 행동을 하면 당근을, 나쁜 행동을 하면 채찍질을 주는 것이 바로 강화 학습입니다. 이 당근과 채찍을 통틀어 강화 학습에서는 \"보상\"이라고 부릅니다. 강화 학습의 가장 중요한 요소 중 하나가 바로 이 보상입니다.\n",
        "\n",
        "![](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/4916/12365.jpeg)\n",
        "\n",
        "강화 학습의 또 다른 중요한 요소는 바로 끊임없이 변화하는 환경의 상태입니다. 어러분이 취하는 행동은 여러분 주변의 환경을 변화시킵니다. 여러분은 이 끊임없이 변화하는 환경 속에서 계속 보상을 최대한 취할 수 있도록 노력해야합니다. 이 과정이 바로 강화 학습의 과정과 많이 닮아있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94yhPKFYquQw"
      },
      "source": [
        "## 강화 학습 예제 경험하기\n",
        "\n",
        "강화 학습이 어떤 문제를 풀 수 있는지 한번 살펴볼까요?\n",
        "\n",
        "사실 강화 학습은 우리 주변에서 흔히 접할 수 있는 대부분의 문제를 다 풀 수 있는 아주 유용한 기술입니다. 로봇을 조종하는 것부터 시작해서, 자율 주행, 주식 투자, 심지어는 최근에 각광받는 ChatGPT 역시 강화 학습 기술이 사용되었습니다. 우리가 일상 생활에서 상벌의 개념을 생각할 수 있는 거의 모든 문제를 강화 학습으로 풀 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3e7d7NMlQPE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U ray[rllib]\n",
        "!pip install gputil\n",
        "!pip install ipython==7.10.0\n",
        "!pip install moviepy\n",
        "!pip install gymnasium[classic-control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk7kE26_lTjF"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import ray\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "\n",
        "def view_video(filename):\n",
        "    video = f'video/{filename}-episode-0.mp4'\n",
        "    mp4 = open(video,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_OKJ8jCqxuQ"
      },
      "source": [
        "### 환경 정의하기\n",
        "\n",
        "오늘 수업 시간에는 강화 학습에서 가장 단순하고 유명한 예제 중 하나인 \"Mountain Car\" 문제를 직접 풀어볼 것입니다. 이 문제에서 여러분의 목표는 산 정상의 깃발에 도달하는 것입니다.\n",
        "여러분은 오른쪽 또는 왼쪽으로 가속하는 페달을 밟거나, 페달을 밟지 않는 선택을 할 수 있습니다.\n",
        "산 정상까지 올라가려면 페달을 적절히 밟아야 합니다. 하지만 산의 경사가 너무 가파르기 때문에, 아무리 밟아도 중력 때문에 올라가지 못할 수도 있습니다.\n",
        "\n",
        "![](https://www.gymlibrary.dev/_images/mountain_car.gif)\n",
        "\n",
        "먼저, 이 환경에서 여러분이 취할 수 있는 행동은 다음과 같이 표현할 수 있습니다:\n",
        "\n",
        "0: 왼쪽으로 가속하는 페달 밟기\n",
        "\n",
        "1: 페달 밟지 않기\n",
        "\n",
        "2: 오른쪽으로 가속하는 페달 밟기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X25O_F-BlbXG",
        "outputId": "effada15-fbad-4ec9-e198-e27d37514971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created env: <TimeLimit<OrderEnforcing<PassiveEnvChecker<MountainCarEnv<MountainCar-v0>>>>>\n",
            "Available actions: Discrete(3)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('MountainCar-v0')\n",
        "print('Created env:', env)\n",
        "print('Available actions:', env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPuDsriJzmUv"
      },
      "source": [
        "다음으로, 이 환경에서 받을 수 있는 상태에 대한 정보는 $s_{t} = (x_{t}, v_{t})$입니다.\n",
        "즉, 여러분이 받는 상태 정보 $s_{t}$는 $x$축 방향에서의 현재 위치 $x_{t}$와 속도 $v_{t}$입니다.\n",
        "\n",
        "$x$축 방향의 좌표는 $(-1.2, 0.6)$ 사이이고, 속도 $v$는 (-0.07, 0.07) 사이입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XWStLA-ldOd",
        "outputId": "a1a4d6ee-22dd-4a91-857f-47f0f4ad06f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "The starting state is: [-0.5098763  0.       ]\n"
          ]
        }
      ],
      "source": [
        "state, _ = env.reset()\n",
        "print('Observation space:', env.observation_space)\n",
        "print('The starting state is:', state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YveQT3A8zmUv"
      },
      "source": [
        "여러분이 어떤 행동을 선택하면, 다음과 같이 환경의 상태가 변화합니다.\n",
        "\n",
        "$v_{t+1} = v_{t} + (\\mathrm{action} - 1) * F - \\cos(3 * x_{t}) * mg$\n",
        "\n",
        "$x_{t+1} = x_{t} + v_{t+1}$\n",
        "\n",
        "여기서 $F$는 페달을 밟을 때 가속하는 힘 $F = 0.001$이고, 중력 $mg = 0.0025$입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKYtgu-blgGL",
        "outputId": "dfa42d66-41bb-4681-d31f-1d9b6ab5f008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: [-0.5109792  -0.00110289]\n",
            "Reward: -1.0\n",
            "Done: False\n"
          ]
        }
      ],
      "source": [
        "action = 0\n",
        "state, reward, done, _, _ = env.step(action)\n",
        "print(f\"State: {state}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Done: {done}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCUIOuWYq0Nq"
      },
      "source": [
        "### 인공지능 정책 정의하기\n",
        "\n",
        "강화 학습에서 정책은 인공지능의 두뇌입니다. 인공지능이 변화하는 환경 속에서 어떻게 행동할지를 결정하는 지침으로 생각할 수 있습니다. 가장 단순한 형태의 정책으로는 여러분이 취할 수 있는 행동 중에서 아무거나 (샘플링하여) 고르는 것이 있겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o18p51sq0lv"
      },
      "outputs": [],
      "source": [
        "class RandomPolicy():\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def __call__(self, state):\n",
        "        return self.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPolicy():\n",
        "    def __init__(self, action_space):\n",
        "      self.action_space = action_space\n",
        "      self.start_point = -0.6\n",
        "\n",
        "    def __call__(self, state):\n",
        "\n",
        "      x, v = state\n",
        "      if v == 0:\n",
        "        if x < self.start_point:\n",
        "          original_action = 0\n",
        "          next_action = 2\n",
        "        elif x > self.start_point:\n",
        "          original_action = 2\n",
        "          next_action = 0\n",
        "        else:\n",
        "          original_action = 1\n",
        "          next_action = 2\n",
        "      else:\n",
        "        if v < 0:\n",
        "          original_action = 0\n",
        "          next_action = 0\n",
        "        elif v > 0:\n",
        "          original_action = 2\n",
        "          next_action = 2\n",
        "        #0: 왼쪽으로 가속하는 페달 밟기\n",
        "        #1: 페달 밟지 않기\n",
        "        #2: 오른쪽으로 가속하는 페달 밟기\n",
        "\n",
        "\n",
        "      return next_action"
      ],
      "metadata": {
        "id": "DH9iVY6QzoW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvVYPQrq3h_"
      },
      "source": [
        "### 환경과 상호작용하기\n",
        "\n",
        "정책이 있다면, 여러분은 주어진 상황에서 어떤 행동을 취해야 할지 결정할 수 있습니다. 그럼 이제 본격적으로 환경 속에 들어가서 직접 부딪혀볼까요?\n",
        "\n",
        "여러분이 어떤 행동을 취하면, 환경은 여러분에게 보상을 줍니다. 이 보상은 여러분이 좋은 행동을 했는지, 나쁜 행동을 했는지 알려주는 값입니다. 그리고 여러분이 취한 행동에 따라 환경은 변화하고, 이 변화한 환경을 여러분은 다시 관찰할 수 있습니다. 그리고 여러분은 또다시 (여러분이 관찰한 결과를 토대로) 어떤 행동을 취할지 결정합니다. 강화 학습은 이 과정을 반복하며 진행됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "mwtH_tezxFYm",
        "outputId": "916edff5-3dd5-4dc9-eef8-9da74ed9105e"
      },
      
      "source": [
        "### BONUS: 강화 학습 에이전트 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxmeXv3xlrgD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "algo = (\n",
        "    PPOConfig()\n",
        "    .rollouts(num_rollout_workers=1)\n",
        "    .resources(num_gpus=1)\n",
        "    .environment(env=\"MountainCar-v0\")\n",
        "    .build()\n",
        ")\n",
        "\n",
        "for i in range(2):\n",
        "    algo.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVJN-_ezzmUx"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, 'video', name_prefix='ppo-agent')\n",
        "\n",
        "state, _ = env.reset()\n",
        "env.start_video_recorder()\n",
        "\n",
        "for t in range(100):\n",
        "    action = algo.compute_single_action(state)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "view_video('ppo-agent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q-1UES5zmUx"
      },
      "source": [
        "# 강화 학습 환경 직접 만져보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph-tmx8wzmUy"
      },
      "source": [
        "### 블랙잭"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomPolicy():\n",
        "    def __init__(self, action_space):\n",
        "        env.action_space = action_space\n",
        "\n",
        "    def __call__(self, state):\n",
        "        return env.action_space.sample()"
      ],
      "metadata": {
        "id": "M0ye-a9r_IeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  def policy(state):\n",
        "  if state[0]+11>21:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "gNwbsg00CCTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jnkIs0xzmUy",
        "outputId": "7b011b1e-23b3-49f6-9101-af028807a043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(2)\n",
            "Action:1\n",
            "State: (12, 4, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (12, 4, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (14, 10, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (14, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (13, 10, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (13, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (14, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (20, 9, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (12, 10, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (12, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (13, 7, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (14, 1, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 7, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (19, 2, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (19, 2, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (15, 2, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (15, 2, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (19, 8, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (19, 8, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (15, 9, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (15, 9, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (14, 9, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (19, 4, 1)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (19, 4, 1)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 5, 1)\n",
            "Reward: 0.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 9, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (15, 3, 1)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (11, 9, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (11, 9, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (14, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (15, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (14, 8, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (11, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (20, 5, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (20, 5, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (21, 6, 1)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (21, 6, 1)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (14, 9, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (20, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (15, 1, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (11, 4, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (15, 10, 1)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (17, 4, 1)\n",
            "Reward: 0.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (17, 5, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 9, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (17, 3, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (18, 5, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 8, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (20, 5, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (11, 2, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (18, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (11, 5, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (11, 5, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (11, 9, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (15, 3, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (18, 7, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (11, 2, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (11, 2, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (20, 7, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (20, 7, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (16, 4, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 9, 0)\n",
            "Reward: 0.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (21, 10, 1)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (20, 6, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (20, 10, 0)\n",
            "Reward: 0.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (21, 9, 1)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (16, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (21, 10, 1)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (18, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (14, 2, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (14, 2, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (17, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (11, 1, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (11, 1, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (21, 9, 1)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (13, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (12, 2, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (12, 2, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (17, 6, 1)\n",
            "Reward: 0.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (20, 3, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (13, 7, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 4, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 8, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (14, 10, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (14, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (13, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (18, 9, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 6, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (13, 6, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (15, 1, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 8, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 1, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (19, 10, 1)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (19, 10, 1)\n",
            "Reward: 0.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 1, 1)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 7, 1)\n",
            "Reward: 0.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 8, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (21, 10, 1)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (18, 1, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (18, 1, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (13, 1, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:1\n",
            "State: (8, 6, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:1\n",
            "State: (11, 6, 0)\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "--------\n",
            "Action:0\n",
            "State: (11, 6, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (20, 10, 0)\n",
            "Reward: 0.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (13, 10, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (19, 3, 0)\n",
            "Reward: 1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (16, 7, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (13, 7, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (16, 4, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (14, 10, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (14, 6, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Discrete(2)\n",
            "Action:0\n",
            "State: (12, 7, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "--------\n",
            "Win :42%\n",
            "lose :50%\n",
            "tie :8%\n"
          ]
        }
      ],
      "source": [
        "def policy(state):\n",
        "  if state[0]+11>21:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "# win = 0\n",
        "# lose = 0\n",
        "\n",
        "# for i in range(10):\n",
        "#   if check_win():\n",
        "#     win += 1\n",
        "#   else:\n",
        "#     lose += 1\n",
        "\n",
        "# print(win)\n",
        "win=0\n",
        "lose=0\n",
        "tie=0\n",
        "for i in range(100):\n",
        "  # Create the Blackjack environment\n",
        "  env = gym.make('Blackjack-v1')\n",
        "  print(f\"{env.action_space}\")\n",
        "\n",
        "  # Reset the environment\n",
        "  state,_ = env.reset()\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      # Choose a random action\n",
        "      action = policy(state)\n",
        "      print(f\"Action:{action}\")\n",
        "\n",
        "      # Take a step in the environment\n",
        "      state, reward, done, _, info = env.step(action)\n",
        "\n",
        "      # Print the current state, reward, and whether the episode is done\n",
        "      print('State:', state)\n",
        "      print('Reward:', reward)\n",
        "      print('Done:', done)\n",
        "      print(\"--------\")\n",
        "  if reward==1:\n",
        "    win+=1\n",
        "  elif reward==-1:\n",
        "    lose+=1\n",
        "  else:\n",
        "    tie+=1\n",
        "\n",
        "print(f\"Win :{win}%\")\n",
        "print(f\"lose :{lose}%\")\n",
        "print(f\"tie :{tie}%\")\n",
        "# Close the environment\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yQrDHu4zmUy"
      },
      "source": [
        "### Tic-Tac-Toe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEssC4tFzmUy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class TicTacToeEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "        self.action_space = spaces.Discrete(9)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 3), dtype=int)\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "        return self.board\n",
        "\n",
        "    def step(self, action):\n",
        "        row = action // 3\n",
        "        col = action % 3\n",
        "\n",
        "        if self.board[row, col] != 0:\n",
        "            return self.board, -1, False, False, {}\n",
        "\n",
        "        self.board[row, col] = self.current_player\n",
        "\n",
        "        if self._check_winner(self.current_player):\n",
        "            return self.board, 1, True, False, {}\n",
        "\n",
        "        if np.count_nonzero(self.board) == 9:\n",
        "            return self.board, 0, True, False, {}\n",
        "\n",
        "        self.current_player = -self.current_player\n",
        "        return self.board, 0, False, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(self.board)\n",
        "\n",
        "    def _check_winner(self, player):\n",
        "        for i in range(3):\n",
        "            if np.all(self.board[i, :] == player) or np.all(self.board[:, i] == player):\n",
        "                return True\n",
        "        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIsQsfoLzmUy"
      },
      "outputs": [],
      "source": [
        "env = TicTacToeEnv()\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = ?????\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Calculator():\n",
        "  def __init__(self):\n",
        "    self.recent = 0\n",
        "\n",
        "  def add(self, x):\n",
        "    self.recent = self.recent + x\n",
        "    return self.recent\n",
        "\n",
        "calculator = Calculator()\n",
        "calculator.add(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0zNOHsmAV-F",
        "outputId": "5a18561d-bfaa-4830-85af-b8c1fb120dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(i)\n",
        "\n",
        "t = 0\n",
        "while t < 10:\n",
        "  print(t)\n",
        "  t += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z6wZVoeGlAV",
        "outputId": "202f9891-4a7b-4aaf-949e-259613f4107a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "I7bQv297zmUx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
